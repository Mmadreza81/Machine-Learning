{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"cell_execution_strategy":"setup","authorship_tag":"ABX9TyMNw7WV3jcSTYLljp6alopi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"3CMUPAoZPrXk","executionInfo":{"status":"ok","timestamp":1708441007161,"user_tz":-210,"elapsed":23,"user":{"displayName":"Mohammadreza Shokri","userId":"16772011889485465213"}}},"outputs":[],"source":["import numpy as np\n","import gym\n","import random\n","\n","env = gym.make(\"FrozenLake-v1\",new_step_api=True)"]},{"cell_type":"code","source":["action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","qtable = np.zeros((state_size, action_size))\n","print(qtable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2nCDRlBRWhb","executionInfo":{"status":"ok","timestamp":1708441008343,"user_tz":-210,"elapsed":15,"user":{"displayName":"Mohammadreza Shokri","userId":"16772011889485465213"}},"outputId":"7f213a89-5f18-4938-9240-20d0a821ebaf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":["total_episodes = 10000        # Total episodes\n","learning_rate = 0.8           # Learning rate\n","max_steps = 99                # Max steps per episode\n","gamma = 0.95                  # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.01             # Exponential decay rate for exploration prob"],"metadata":{"id":"V-mI9N8TSZmH","executionInfo":{"status":"ok","timestamp":1708441010641,"user_tz":-210,"elapsed":459,"user":{"displayName":"Mohammadreza Shokri","userId":"16772011889485465213"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# List of rewards\n","rewards = []\n","\n","# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","\n","    for step in range(max_steps):\n","        # 3. Choose an action a in the current world state (s)\n","        ## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","\n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","\n","        total_rewards += reward\n","\n","        # Our new state is state\n","        state = new_state\n","\n","        # If done (if we're dead) : finish episode\n","        if done == True:\n","            break\n","\n","    episode += 1\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    rewards.append(total_rewards)\n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"JWkFNyfGUcIx","executionInfo":{"status":"error","timestamp":1708441014543,"user_tz":-210,"elapsed":416,"user":{"displayName":"Mohammadreza Shokri","userId":"16772011889485465213"}},"outputId":"b021d810-e8b9-4b6f-a34a-1e18d84957ac"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 4)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4dcd80a22b2b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Take the action (a) and observe the outcome state(s') and reward (r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"]}]},{"cell_type":"code","source":["env.reset()\n","\n","for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"*\" * 40)\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        env.render()\n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","\n","        new_state, reward, done, info = env.step(action)\n","\n","        if done:\n","            break\n","        state = new_state\n","env.close()"],"metadata":{"id":"LYJQkdftgpwt"},"execution_count":null,"outputs":[]}]}